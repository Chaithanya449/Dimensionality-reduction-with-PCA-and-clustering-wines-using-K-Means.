# -*- coding: utf-8 -*-
"""Dimensionality reduction with PCA and clustering wines using K-Means..ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UdKbJtEWnIPURpaqYjZV8EisdvV9gCrO
"""

#Importing necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

# Loaading the dataset
df =pd.read_csv('wine.csv')

df.head()

df.info()

# getting the row and column counts
df.shape # So dataset has 178 rows and 14 columns

# Obtaining summary statistics
df.describe()

df['Type'].value_counts()

df.isnull().sum() # There is no null values so no need of applying imputation methods to fill null values

# Dropping type column as its not required for the plotting
feature_cols = df.drop( 'Type',axis =1,inplace = True)

for feature_cols in df.columns:
    plt.figure(figsize = (11,6))
    sns.set_style('darkgrid')
    sns.histplot(df[feature_cols],kde = True,bins = 20)
    plt.tight_layout()
    plt.show()
#sns.histplot(df[''],kde = True,bins = 20)
#plt.show()

for feature_cols in df.columns:
    plt.figure(figsize = (11,6))
    sns.set_style('darkgrid')
    sns.boxplot(df[feature_cols])
    plt.tight_layout()
    plt.show()

# Calculating correlation to understand the relationship betweeen the features
correlation_matrtix = df.corr()
plt.figure(figsize =(11,6))
sns.heatmap(correlation_matrtix,annot = True,cmap ='coolwarm',fmt='.2f')  # Heatmap is best visualization to understand correlation for all the features

# STANDARDIZATION
# Standardizing the data before pca is very important because without standardizing pca will think bigger numbers are more important than smaller numbers.
# standardization method used before pca instead of min_max because it will compress the variance here info is lost as pca entirely works on variance.
from sklearn.preprocessing import StandardScaler # Standardization will scale the features such that mean = 0 and Standard deviation is 1.
scaler =StandardScaler()
scaled_df = scaler.fit_transform(df)
scaled_df = pd.DataFrame(scaled_df,columns = df.columns)
scaled_df.head()

from sklearn.decomposition import PCA
pca = PCA()
pca_df = pca.fit(scaled_df)

# obtaining optimal number of pricipal components
EVR = pca.explained_variance_ratio_
cumul_EVR = np.cumsum(EVR)
plt.figure(figsize = (11,6))
plt.plot(cumul_EVR,marker = 'o',linestyle = '--')
plt.title('Total variance explained by each principal component')
plt.xlabel('principal component')
plt.ylabel('Total variance')
plt.legend(['cumul_EVR'])
plt.tight_layout()
plt.show()

plt.figure(figsize = (11,6))
plt.plot(EVR,marker = 'o',linestyle = '--')
plt.title('percentage of variance by each principal component')
plt.xlabel('principal component')
plt.ylabel('percentage of variance')
plt.legend(['EVR'])
plt.tight_layout()
plt.show()

"""13 features are created as  13 principal components.1st pc capturing the highest variance that is 36% and 2nd pc with 19%."""

# Dimensionality reduction
# Transforming and creating a data using 2 principal components
# The Two pc's capture almost 55% of the variance
n_components = 2
pca = PCA(n_components)
df_1 = pca.fit_transform(df)
# creating a new dataframe
df_2 = pd.DataFrame(data = df_1,columns = ['PC1','PC2'])
df_2.head()

# Applying k-means clustering algorithm on the original dataframe
from sklearn.cluster import KMeans
df_3 = pd.read_csv('wine.csv')
df_3.head()
df_3.drop('Type',axis = 1,inplace =True)
df_3.head()
n_clusters = 3 # As we need to group the wine based on 3 types the no of clusters are 3
kmeans = KMeans(n_clusters = n_clusters)
kmeans_labels = kmeans.fit_predict(scaled_df)
# visualizing the original dataframe is not possible as there are 13 features in the df
# visualizing the original dataframe using pca
pca = PCA(n_components =2)
pca_df=pca.fit_transform(df_3)
# creating a new dataframe of pca
pca_df = pd.DataFrame(data=pca_df,columns =['PC1','PC2'])
pca_df.head()
plt.figure(figsize =(11,6))
sns.scatterplot(x='PC1',y='PC2',data = pca_df,hue = kmeans_labels,palette = 'viridis',)
plt.legend(loc='best')
plt.tight_layout()
plt.show()

pca_df.shape

from sklearn.metrics import silhouette_score,adjusted_rand_score
s_score = silhouette_score(df_3,kmeans_labels) # Sihouette score directly looks at the data and the cluster labels and judges the score
print(f'Silhouette_score:{s_score:.3f}')
# Sihouette score does not require the actual answers i.e.,Type column
# adjusted_rand_score is a metric that requires the actual answers and the cluster labels compares and returns the score
df_4 = pd.read_csv('wine.csv')
true_labels = df_4['Type']
AR_score = adjusted_rand_score(true_labels,kmeans_labels)
print(f'adjusted_rand_score:{AR_score:.3f}')

# Applying clustering onto the pca transformed data
df_5 = pd.read_csv('wine.csv')
df_5.drop('Type',axis = 1,inplace=True)
model_pca = PCA(n_components=2)
pca_df_1 = model_pca.fit_transform(df_5)
# Creating a new dataframe of pca transformed data
pca_df_1 = pd.DataFrame(data=pca_df_1,columns=['Principal_component_1','Principal_component_2'])
pca_df_1.head()
# Applying kmeans clustering

clusters=3
kmeans_pca = KMeans(n_clusters=clusters,random_state =42)
k_labels=kmeans_pca.fit_predict(pca_df_1)
# Visualizing the results
sns.scatterplot(x = 'Principal_component_1',y = 'Principal_component_2',data = pca_df_1,hue = k_labels,palette='viridis')
plt.legend(loc='best')
plt.tight_layout()
plt.show()

from sklearn.metrics import silhouette_score,adjusted_rand_score
si_score = silhouette_score(df_5,k_labels)
print(f'silhouette_score:{si_score:.3f}')
df_6 =pd.read_csv('wine.csv')
True_labels = df_6['Type']
AR_score = adjusted_rand_score(True_labels,k_labels)
print(f'adjusted_rand_score:{AR_score:.3f}')

"""# Comaparing the results of both the clustering tasks
Metric	   Task 3 (On 13 Original Features)	Task 4 (On 2 PCA Features)
Adjusted_R	      0.897 (Excellent)	                 0.352 (Poor)
Silhouette Score	0.194 (Modest)	                   0.560 (Good)
 The goal was to find the type of the wine based on 13 chemical properties.clustering is a great choice here as it groups the data points based on similarities and closeness.
Similarities and differences in the scores:
  adjusted_r_score has increased that is the accuracy was high than the applied clustering algorithm on the pca transformed data.adjusted_r_score focuses on the accuracy of whether the data points were perfectly assigned to the cluster.The decresed of score for pca transformed data might be due to the dimensionality reduction as some of the information is lost while dimensionality reduction process.hence its clear that clustering algorithm requires complete data that is all the 13 features which are there in the original dataset give higher accuracy in  grouping the wine type based on the similarity.There is a difference in the silhouette scores of both the tasks.silhouette score primarily focusses on measuring the cluster shape of whether it is dense and well seperated or the shape of the clusters is overlapped to each other or not.The score for the task 3 with all the features is less than that of the score of the task 4 which is of pca transformed data.Here this might be due to reason,the original dataset was having 13 fatures and it made clustering algorithm difficult to separate the clusters perfectly and task 4 of the clustering process had a higher silhouette score that means clusters are not much overlapped this is because pca transformed data is having only 2 dimensions due to dimensionality reduction process and there is a loss of info which made the pca trasformed data to achieve a  higher score than the clusterig applied on the original dataframe
  The impact of dimensionality reduction on the accuracy:The accuracy was clearly dropped on the pca trnasformed data.During dimension reduction the information is lost though the two components capture most of the information some of it is lost which would be the reason of dropped accuracy.clustering algorithm did not do well to group the data points into groups to form clusters on the pca transformed data as the information is not complete.clustering algorithm performed very well with 90% accuracy on the original dataframe this is because it has all the 13 features the complete information needed to know the type of the wine based on the 13 chemical properties.

Key Findings and Insights from the Assignment:
The accuracy of clustering increases when there is more data.Dimensionality reduction comes at a cost when dimensions are reduced some of the information is lost leading to lesser accuracy.PCA can create a better visual as it can reduce features in our assignment there were 13 features which can be messy and difficult to visualize without pca.PCA can create beautifull visuals where then are more no of features in the dataset but produces lower  accuracy.
 Practical Implications of PCA and Clustering:
 Customer Segmentation (Clustering),Image Compression (PCA),Biological Research (Clustering & PCA),Recommendation Engines (Clustering)
  Recommendations for When to Use Each Technique:
  when the primary goal is to achieve higher accuracy then clustering must be done on the original data.When the primary  goal is to group something quickly then clustering on pca transformed data has to be used.As pca reduces dimensions grouping can be very fast. when there are more dimensions or features the grouping can be slow.PCA technique can also be used when the goal is to create visuals.Finally when the primary goal is speed and when there is a large dataset then pca can be used.
"""